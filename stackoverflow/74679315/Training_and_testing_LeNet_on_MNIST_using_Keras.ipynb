{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "[![View on GitHub][github-badge]][github-notebook] [![Open In Colab][colab-badge]][colab-notebook] [![Open in Binder][binder-badge]][binder-notebook]\n",
        "\n",
        "[github-badge]: https://img.shields.io/badge/View-on%20GitHub-blue?logo=GitHub\n",
        "[colab-badge]: https://colab.research.google.com/assets/colab-badge.svg\n",
        "[binder-badge]: https://static.mybinder.org/badge_logo.svg\n",
        "\n",
        "[github-notebook]: https://github.com/mbrukman/stackexchange-answers/blob/main/stackoverflow/74679315/Training_and_testing_LeNet_on_MNIST_using_Keras.ipynb\n",
        "[colab-notebook]: https://colab.research.google.com/github/mbrukman/stackexchange-answers/blob/main/stackoverflow/74679315/Training_and_testing_LeNet_on_MNIST_using_Keras.ipynb\n",
        "[binder-notebook]: https://mybinder.org/v2/gh/mbrukman/stackexchange-answers/main?filepath=stackoverflow/74679315/Training_and_testing_LeNet_on_MNIST_using_Keras.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "This notebooks is helping investigate and answer [this Stack Overflow question][1]. Let's start by downloading the same MNIST dataset that's used in the question.\n",
        "\n",
        "[1]: https://stackoverflow.com/q/74679315/3618671"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "MNIST_PNG=\"mnist_png.tar.gz\"\n",
        "if ! [ -e \"${MNIST_PNG}\" ]; then\n",
        "  curl -sO \"https://raw.githubusercontent.com/myleott/mnist_png/master/${MNIST_PNG}\"\n",
        "fi\n",
        "\n",
        "if ! [ -d \"mnist_png\" ]; then\n",
        "  tar zxf \"${MNIST_PNG}\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "Optionally, you can uncomment the command below to simulate missing data, as the SO question shows that it only has 7 classes of training inputs instead of 10.\n",
        "\n",
        "For example, with the classes {7, 8, 9} deleted, we find that the training accuracy is still rather high:\n",
        "\n",
        "* loss: 0.1568\n",
        "* sparse_categorical_accuracy: 0.9541\n",
        "* val_loss: 0.0616\n",
        "* val_sparse_categorical_accuracy: 0.9801\n",
        "\n",
        "while the test accuracy is much lower:\n",
        "\n",
        "* loss: 2.1828\n",
        "* sparse_categorical_accuracy: 0.6873"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# (Optional) Delete some of the training data dirs to simulate missing data.\n",
        "# rm -rf mnist_png/training/[789]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "Now that we've downloaded the MNIST dataset, let's see what the sizes of the images are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PIL modes: dict_items([('L', 70000)])\n",
            "PIL sizes: dict_items([((28, 28), 70000)])\n",
            "matplotlib sizes:  dict_items([((28, 28), 70000)])\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import glob\n",
        "import matplotlib\n",
        "import PIL\n",
        "\n",
        "pil_modes = defaultdict(int)\n",
        "pil_sizes = defaultdict(int)\n",
        "mpl_sizes = defaultdict(int)\n",
        "\n",
        "files = (glob.glob('mnist_png/training/[0-9]/*.png') +\n",
        "         glob.glob('mnist_png/testing/[0-9]/*.png'))\n",
        "for file in files:\n",
        "    with PIL.Image.open(file) as pil_image:\n",
        "        pil_modes[pil_image.mode] += 1\n",
        "        pil_sizes[pil_image.size] += 1\n",
        "        mpl_image = matplotlib.image.pil_to_array(pil_image)\n",
        "        mpl_sizes[mpl_image.shape] += 1\n",
        "\n",
        "print('PIL modes:', pil_modes.items())\n",
        "print('PIL sizes:', pil_sizes.items())\n",
        "print('matplotlib sizes: ', mpl_sizes.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "Both PIL and `matplotlib` agree that the image size is `(28, 28)`.\n",
        "\n",
        "The PIL image mode `L` is greyscale, so there's only 1 color channel. If the images were RGB, it would have 3 channels, but MNIST has only 1 channel.\n",
        "\n",
        "So, if the MNIST dataset has $28 \\times 28$ images, but the LeNet model says it takes $32 \\times 32$ images, how does that work? Well, we can either adjust the images (training, validation, and testing datasets):\n",
        "\n",
        "* resize the images before passing them to the LeNet model\n",
        "* pad the images with zeroes before passing them to the LeNet model\n",
        "\n",
        "or by handling this in the model itself, e.g.,\n",
        "\n",
        "* use the `padding` feature of the [`Conv2D` layer][conv2d] (via `padding='same'` parameter) to zero-pad images during training and testing\n",
        "\n",
        "Note that these are mutually-exclusive options, so we can only do either of the following, but not both:\n",
        "\n",
        "1. resize images when loading with [image_dataset_from_directory()][image_dataset_from_directory] function by specifying `image_size=(32, 32)`\n",
        "1. specify `image_size(28, 28)` when loading as it is their native size, and then use `Conv2D(..., padding='same')` in the model to zero-pad dynamically\n",
        "\n",
        "Below, we're using option $(2)$.\n",
        "\n",
        "Additionally, we need to consider the color channels. Since the MNIST images have a single color channel, the source images have the dimensions `(28, 28, 1)`. However, since [`image_dataset_from_directory()`][image_dataset_from_directory] has a default parameter `color_mode='rgb'`, if we do nothing, it will auto-convert the image from 1 color channel (grayscale) to 3 color channels (RGB), but we don't want that, so we have to explicitly specify `color_mode='grayscale'` below.\n",
        "\n",
        "[conv2d]: https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "[image_dataset_from_directory]: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 60000 files belonging to 10 classes.\n",
            "Using 48000 files for training.\n",
            "Found 60000 files belonging to 10 classes.\n",
            "Using 12000 files for validation.\n",
            "Found 10000 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# This code was adapted from SO question: https://stackoverflow.com/q/74679315\n",
        "# and adjusted with the change as described above.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    'mnist_png/training/',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(28, 28),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=100)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    'mnist_png/training/',\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(28, 28),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=100)\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    'mnist_png/testing/',\n",
        "    seed=123,\n",
        "    image_size=(28, 28),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "Here, we see that there are 60000 training images and 10000 test images across 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "As an aside, since we're already using Keras, there's a much easier way to get the MNIST dataset directly [via Keras][1] in just a single line:\n",
        "\n",
        "```python\n",
        "from tensorflow import keras\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "```\n",
        "\n",
        "It's the same data as in the PNG repo above, and the images are also of the size $28 \\times 28$ and they're greyscale, with 1 color channel.\n",
        "\n",
        "[keras-mnist]: https://keras.io/api/datasets/mnist/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "Let's define the LeNet-5 model as per the SO question, using `AveragePooling2D` in place of the custom Subsampling layer the paper talks about, since it's not provided by Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import Input, Sequential\n",
        "from keras.layers import Activation, AveragePooling2D, Conv2D, Dense, Flatten\n",
        "\n",
        "\n",
        "tanh = keras.activations.tanh\n",
        "softmax = keras.activations.softmax\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(28, 28, 1)),\n",
        "    Conv2D(filters=6, kernel_size=(5, 5), padding='same', activation=tanh, name='C1'),\n",
        "    AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='S2'),\n",
        "    Activation(tanh, name='S2_act'),\n",
        "    Conv2D(filters=16, kernel_size=(5, 5), activation=tanh, name='C3'),\n",
        "    AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='S4'),\n",
        "    Activation(tanh, name='S4_act'),\n",
        "    Conv2D(filters=120, kernel_size=(5, 5), activation=tanh, name='C5'),\n",
        "    Flatten(name='Flatten'),\n",
        "    Dense(84, activation=tanh, name='F6'),\n",
        "    Dense(10, activation=softmax, name='Output'),\n",
        "], name='LeNet-5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": null
      },
      "source": [
        "Above, we constructed the LeNet model using `AveragePooling2D` layer with `tanh` activation. We can also use `MaxPooling2D` layer instead, or implement the [`Subsampling`][subsampling] layer as described in the paper.\n",
        "\n",
        "[subsampling]: https://github.com/mbrukman/reimplementing-ml-papers/blob/main/lenet/subsampling.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"LeNet-5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " C1 (Conv2D)                 (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " S2 (AveragePooling2D)       (None, 14, 14, 6)         0         \n",
            "                                                                 \n",
            " S2_act (Activation)         (None, 14, 14, 6)         0         \n",
            "                                                                 \n",
            " C3 (Conv2D)                 (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " S4 (AveragePooling2D)       (None, 5, 5, 16)          0         \n",
            "                                                                 \n",
            " S4_act (Activation)         (None, 5, 5, 16)          0         \n",
            "                                                                 \n",
            " C5 (Conv2D)                 (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " Flatten (Flatten)           (None, 120)               0         \n",
            "                                                                 \n",
            " F6 (Dense)                  (None, 84)                10164     \n",
            "                                                                 \n",
            " Output (Dense)              (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "480/480 [==============================] - 22s 26ms/step - loss: 0.2199 - sparse_categorical_accuracy: 0.9381 - val_loss: 0.0845 - val_sparse_categorical_accuracy: 0.9748\n",
            "Epoch 2/10\n",
            "480/480 [==============================] - 13s 27ms/step - loss: 0.0760 - sparse_categorical_accuracy: 0.9770 - val_loss: 0.0720 - val_sparse_categorical_accuracy: 0.9783\n",
            "Epoch 3/10\n",
            "480/480 [==============================] - 13s 26ms/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.0577 - val_sparse_categorical_accuracy: 0.9827\n",
            "Epoch 4/10\n",
            "480/480 [==============================] - 13s 27ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.0525 - val_sparse_categorical_accuracy: 0.9843\n",
            "Epoch 5/10\n",
            "480/480 [==============================] - 13s 27ms/step - loss: 0.0409 - sparse_categorical_accuracy: 0.9875 - val_loss: 0.0501 - val_sparse_categorical_accuracy: 0.9846\n",
            "Epoch 6/10\n",
            "480/480 [==============================] - 14s 28ms/step - loss: 0.0339 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0438 - val_sparse_categorical_accuracy: 0.9863\n",
            "Epoch 7/10\n",
            "480/480 [==============================] - 13s 28ms/step - loss: 0.0299 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.0446 - val_sparse_categorical_accuracy: 0.9869\n",
            "Epoch 8/10\n",
            "480/480 [==============================] - 14s 28ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.0437 - val_sparse_categorical_accuracy: 0.9871\n",
            "Epoch 9/10\n",
            "480/480 [==============================] - 15s 31ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9932 - val_loss: 0.0406 - val_sparse_categorical_accuracy: 0.9872\n",
            "Epoch 10/10\n",
            "480/480 [==============================] - 13s 26ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.0415 - val_sparse_categorical_accuracy: 0.9880\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1cdf6067c0>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 2s 32ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9871\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.042519859969615936, 0.9871000051498413]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}